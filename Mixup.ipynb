{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mixup.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dToebnl168KQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################################\n",
        "# Title= CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features \n",
        "# Author= Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon,\n",
        "# Booktitle = International Conference on Computer Vision (ICCV)\n",
        "# Year=2019\n",
        "# Availability: https://github.com/clovaai/CutMix-PyTorch\n",
        "################################################################################\n",
        "\n",
        "###############################################################\n",
        "#Title: mixup: Beyond Empirical Risk Minimization\n",
        "#Author: Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz\n",
        "#Journal: International Conference on Learning Representations\n",
        "#Date: 2018\n",
        "#Availability: https://github.com/facebookresearch/mixup-cifar10\n",
        "###############################################################\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fklAgLdy9vqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3,   64,  3)\n",
        "        self.conv2 = nn.Conv2d(64,  128, 3)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9CkzQAWq1WW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    current_LR = get_learning_rate(optimizer)[0]\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        input = input.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "        r = np.random.rand(1)\n",
        "\n",
        "        # Apply Mixup to the input data\n",
        "        if use_cuda:\n",
        "              input, target = input.cuda(), target.cuda()\n",
        "\n",
        "        input, targets_a, targets_b, lam_mu = mixup_data(input, target, alpha)\n",
        "\n",
        "        input_var = torch.autograd.Variable(input, requires_grad=True)\n",
        "        target_a_var = torch.autograd.Variable(targets_a)\n",
        "        target_b_var = torch.autograd.Variable(targets_b)\n",
        "        output = model(input_var)\n",
        "        loss = mixup_criterion(criterion, output, targets_a, targets_b, lam_mu)\n",
        "\n",
        "        \n",
        "        # measure accuracy and record loss\n",
        "        err1, err5 = accuracy(output.data, target, topk=(1, 5))\n",
        "\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(err1.item(), input.size(0))\n",
        "        top5.update(err5.item(), input.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # if i % print_freq == 0 and verbose == True:\n",
        "        #     print('Epoch: [{0}/{1}][{2}/{3}]\\t'\n",
        "        #           'LR: {LR:.6f}\\t'\n",
        "        #           'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "        #           'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "        #           'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "        #           'Top 1-err {top1.val:.4f} ({top1.avg:.4f})\\t'.format(\n",
        "        #         epoch, epochs, i, len(train_loader), LR=current_LR, batch_time=batch_time,\n",
        "        #         data_time=data_time, loss=losses, top1=top1))\n",
        "\n",
        "    print('* Epoch: [{0}/{1}]\\t Top 1-err {top1.avg:.3f}  Top 5-err {top5.avg:.3f}\\t Train Loss {loss.avg:.3f}'.format(\n",
        "        epoch, epochs, top1=top1, top5=top5, loss=losses))\n",
        "\n",
        "    return losses.avg\n",
        "\n",
        "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = np.int(W * cut_rat)\n",
        "    cut_h = np.int(H * cut_rat)\n",
        "\n",
        "    # uniform\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion, epoch):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(val_loader):\n",
        "        target = target.cuda()\n",
        "\n",
        "        input_var = torch.autograd.Variable(input)\n",
        "        target_var = torch.autograd.Variable(target)\n",
        "        output = model(input_var)\n",
        "        loss = criterion(output, target_var)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        err1, err5 = accuracy(output.data, target, topk=(1, 5))\n",
        "\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "\n",
        "        top1.update(err1.item(), input.size(0))\n",
        "        top5.update(err5.item(), input.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # if i % print_freq == 0 and verbose == True:\n",
        "        #     print('Test (on val set): [{0}/{1}][{2}/{3}]\\t'\n",
        "        #           'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "        #           'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "        #           'Top 1-err {top1.val:.4f} ({top1.avg:.4f})\\t'.format(\n",
        "        #            epoch, epochs, i, len(val_loader), batch_time=batch_time, loss=losses,\n",
        "        #            top1=top1))\n",
        "\n",
        "    print('* Epoch: [{0}/{1}]\\t Top 1-err {top1.avg:.3f}  Top 5-err {top5.avg:.3f}\\t Test Loss {loss.avg:.3f}'.format(\n",
        "        epoch, epochs, top1=top1, top5=top5, loss=losses))\n",
        "    return top1.avg, top5.avg, losses.avg\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    global lr\n",
        "    lr = lr * (0.1 ** (epoch // (epochs * 0.5))) * (0.1 ** (epoch // (epochs * 0.75)))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def get_learning_rate(optimizer):\n",
        "    lr = []\n",
        "    for param_group in optimizer.param_groups:\n",
        "        lr += [param_group['lr']]\n",
        "    return lr\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "        wrong_k = batch_size - correct_k\n",
        "        res.append(wrong_k.mul_(100.0 / batch_size))\n",
        "\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fk8RVjo1PQW",
        "colab_type": "code",
        "outputId": "4e26b64e-ba19-40ba-f19f-202971b59ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size=100\n",
        "numberofclass = 10\n",
        "depth=200\n",
        "alpha=240\n",
        "lr=0.1\n",
        "epochs=300\n",
        "beta=1.0\n",
        "print_freq=10\n",
        "verbose=True\n",
        "workers=4\n",
        "momentum=0.9\n",
        "weight_decay=1e-4\n",
        "best_err1 = 100\n",
        "best_err5 = 100\n",
        "use_cuda = torch.cuda.is_available()\n",
        "normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                          std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
        "\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                datasets.CIFAR10('../data', train=True, download=True, transform=transform_train),\n",
        "                batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "                datasets.CIFAR10('../data', train=False, transform=transform_test),\n",
        "                batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "\n",
        "model = CNN()\n",
        "model = torch.nn.DataParallel(model).cuda()\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n",
        "cudnn.benchmark = True\n",
        "\n",
        "for epoch in range(0, 300):\n",
        "\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "    # train for one epoch\n",
        "    train_loss = train(train_loader, model, criterion, optimizer, epoch)\n",
        "\n",
        "    # evaluate on validation set\n",
        "    err1, err5, val_loss = validate(val_loader, model, criterion, epoch)\n",
        "\n",
        "    # remember best prec@1 and save checkpoint\n",
        "    is_best = err1 <= best_err1\n",
        "    best_err1 = min(err1, best_err1)\n",
        "    if is_best:\n",
        "        best_err5 = err5\n",
        "\n",
        "    print('Current best accuracy (top-1 and 5 error):', best_err1, best_err5)\n",
        "print('Best accuracy (top-1 and 5 error):', best_err1, best_err5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "* Epoch: [0/300]\t Top 1-err 77.826  Top 5-err 27.372\t Train Loss 2.098\n",
            "* Epoch: [0/300]\t Top 1-err 58.190  Top 5-err 12.020\t Test Loss 1.599\n",
            "Current best accuracy (top-1 and 5 error): 58.19 12.02\n",
            "* Epoch: [1/300]\t Top 1-err 71.348  Top 5-err 20.540\t Train Loss 1.971\n",
            "* Epoch: [1/300]\t Top 1-err 52.830  Top 5-err 8.500\t Test Loss 1.463\n",
            "Current best accuracy (top-1 and 5 error): 52.83 8.5\n",
            "* Epoch: [2/300]\t Top 1-err 67.748  Top 5-err 18.246\t Train Loss 1.903\n",
            "* Epoch: [2/300]\t Top 1-err 43.270  Top 5-err 4.870\t Test Loss 1.253\n",
            "Current best accuracy (top-1 and 5 error): 43.27 4.87\n",
            "* Epoch: [3/300]\t Top 1-err 66.156  Top 5-err 16.874\t Train Loss 1.863\n",
            "* Epoch: [3/300]\t Top 1-err 40.760  Top 5-err 5.930\t Test Loss 1.263\n",
            "Current best accuracy (top-1 and 5 error): 40.76 5.93\n",
            "* Epoch: [4/300]\t Top 1-err 65.424  Top 5-err 16.046\t Train Loss 1.838\n",
            "* Epoch: [4/300]\t Top 1-err 36.790  Top 5-err 4.020\t Test Loss 1.111\n",
            "Current best accuracy (top-1 and 5 error): 36.79 4.02\n",
            "* Epoch: [5/300]\t Top 1-err 64.422  Top 5-err 15.484\t Train Loss 1.820\n",
            "* Epoch: [5/300]\t Top 1-err 39.540  Top 5-err 5.040\t Test Loss 1.163\n",
            "Current best accuracy (top-1 and 5 error): 36.79 4.02\n",
            "* Epoch: [6/300]\t Top 1-err 64.008  Top 5-err 15.196\t Train Loss 1.808\n",
            "* Epoch: [6/300]\t Top 1-err 41.000  Top 5-err 5.100\t Test Loss 1.190\n",
            "Current best accuracy (top-1 and 5 error): 36.79 4.02\n",
            "* Epoch: [7/300]\t Top 1-err 63.362  Top 5-err 14.834\t Train Loss 1.792\n",
            "* Epoch: [7/300]\t Top 1-err 36.360  Top 5-err 4.230\t Test Loss 1.126\n",
            "Current best accuracy (top-1 and 5 error): 36.36 4.23\n",
            "* Epoch: [8/300]\t Top 1-err 62.770  Top 5-err 14.478\t Train Loss 1.788\n",
            "* Epoch: [8/300]\t Top 1-err 33.260  Top 5-err 4.270\t Test Loss 1.049\n",
            "Current best accuracy (top-1 and 5 error): 33.26 4.27\n",
            "* Epoch: [9/300]\t Top 1-err 62.500  Top 5-err 14.236\t Train Loss 1.780\n",
            "* Epoch: [9/300]\t Top 1-err 33.620  Top 5-err 3.830\t Test Loss 1.038\n",
            "Current best accuracy (top-1 and 5 error): 33.26 4.27\n",
            "* Epoch: [10/300]\t Top 1-err 62.314  Top 5-err 14.128\t Train Loss 1.773\n",
            "* Epoch: [10/300]\t Top 1-err 35.070  Top 5-err 4.460\t Test Loss 1.088\n",
            "Current best accuracy (top-1 and 5 error): 33.26 4.27\n",
            "* Epoch: [11/300]\t Top 1-err 61.648  Top 5-err 13.478\t Train Loss 1.761\n",
            "* Epoch: [11/300]\t Top 1-err 34.020  Top 5-err 3.490\t Test Loss 1.044\n",
            "Current best accuracy (top-1 and 5 error): 33.26 4.27\n",
            "* Epoch: [12/300]\t Top 1-err 62.030  Top 5-err 13.772\t Train Loss 1.758\n",
            "* Epoch: [12/300]\t Top 1-err 31.090  Top 5-err 2.970\t Test Loss 0.994\n",
            "Current best accuracy (top-1 and 5 error): 31.09 2.97\n",
            "* Epoch: [13/300]\t Top 1-err 61.834  Top 5-err 13.758\t Train Loss 1.755\n",
            "* Epoch: [13/300]\t Top 1-err 33.150  Top 5-err 3.230\t Test Loss 1.034\n",
            "Current best accuracy (top-1 and 5 error): 31.09 2.97\n",
            "* Epoch: [14/300]\t Top 1-err 61.442  Top 5-err 13.352\t Train Loss 1.749\n",
            "* Epoch: [14/300]\t Top 1-err 31.400  Top 5-err 3.760\t Test Loss 1.011\n",
            "Current best accuracy (top-1 and 5 error): 31.09 2.97\n",
            "* Epoch: [15/300]\t Top 1-err 61.774  Top 5-err 13.708\t Train Loss 1.747\n",
            "* Epoch: [15/300]\t Top 1-err 34.570  Top 5-err 3.600\t Test Loss 1.050\n",
            "Current best accuracy (top-1 and 5 error): 31.09 2.97\n",
            "* Epoch: [16/300]\t Top 1-err 61.394  Top 5-err 13.452\t Train Loss 1.744\n",
            "* Epoch: [16/300]\t Top 1-err 32.880  Top 5-err 3.920\t Test Loss 1.009\n",
            "Current best accuracy (top-1 and 5 error): 31.09 2.97\n",
            "* Epoch: [17/300]\t Top 1-err 60.884  Top 5-err 13.244\t Train Loss 1.737\n",
            "* Epoch: [17/300]\t Top 1-err 31.040  Top 5-err 3.280\t Test Loss 0.980\n",
            "Current best accuracy (top-1 and 5 error): 31.04 3.28\n",
            "* Epoch: [18/300]\t Top 1-err 60.658  Top 5-err 13.088\t Train Loss 1.738\n",
            "* Epoch: [18/300]\t Top 1-err 33.040  Top 5-err 3.290\t Test Loss 1.013\n",
            "Current best accuracy (top-1 and 5 error): 31.04 3.28\n",
            "* Epoch: [19/300]\t Top 1-err 60.706  Top 5-err 13.230\t Train Loss 1.736\n",
            "* Epoch: [19/300]\t Top 1-err 34.030  Top 5-err 3.900\t Test Loss 1.061\n",
            "Current best accuracy (top-1 and 5 error): 31.04 3.28\n",
            "* Epoch: [20/300]\t Top 1-err 60.946  Top 5-err 13.214\t Train Loss 1.734\n",
            "* Epoch: [20/300]\t Top 1-err 31.830  Top 5-err 3.200\t Test Loss 1.001\n",
            "Current best accuracy (top-1 and 5 error): 31.04 3.28\n",
            "* Epoch: [21/300]\t Top 1-err 60.404  Top 5-err 12.792\t Train Loss 1.726\n",
            "* Epoch: [21/300]\t Top 1-err 31.090  Top 5-err 3.710\t Test Loss 0.979\n",
            "Current best accuracy (top-1 and 5 error): 31.04 3.28\n",
            "* Epoch: [22/300]\t Top 1-err 60.432  Top 5-err 13.008\t Train Loss 1.728\n",
            "* Epoch: [22/300]\t Top 1-err 31.730  Top 5-err 3.560\t Test Loss 1.002\n",
            "Current best accuracy (top-1 and 5 error): 31.04 3.28\n",
            "* Epoch: [23/300]\t Top 1-err 60.674  Top 5-err 12.886\t Train Loss 1.729\n",
            "* Epoch: [23/300]\t Top 1-err 30.880  Top 5-err 3.240\t Test Loss 0.981\n",
            "Current best accuracy (top-1 and 5 error): 30.88 3.24\n",
            "* Epoch: [24/300]\t Top 1-err 60.606  Top 5-err 12.908\t Train Loss 1.725\n",
            "* Epoch: [24/300]\t Top 1-err 29.600  Top 5-err 3.180\t Test Loss 0.954\n",
            "Current best accuracy (top-1 and 5 error): 29.6 3.18\n",
            "* Epoch: [25/300]\t Top 1-err 60.102  Top 5-err 12.554\t Train Loss 1.715\n",
            "* Epoch: [25/300]\t Top 1-err 32.570  Top 5-err 3.290\t Test Loss 1.008\n",
            "Current best accuracy (top-1 and 5 error): 29.6 3.18\n",
            "* Epoch: [26/300]\t Top 1-err 59.944  Top 5-err 12.530\t Train Loss 1.714\n",
            "* Epoch: [26/300]\t Top 1-err 33.910  Top 5-err 3.800\t Test Loss 1.041\n",
            "Current best accuracy (top-1 and 5 error): 29.6 3.18\n",
            "* Epoch: [27/300]\t Top 1-err 60.552  Top 5-err 12.880\t Train Loss 1.721\n",
            "* Epoch: [27/300]\t Top 1-err 31.200  Top 5-err 3.790\t Test Loss 1.005\n",
            "Current best accuracy (top-1 and 5 error): 29.6 3.18\n",
            "* Epoch: [28/300]\t Top 1-err 60.036  Top 5-err 12.504\t Train Loss 1.717\n",
            "* Epoch: [28/300]\t Top 1-err 29.740  Top 5-err 3.970\t Test Loss 0.969\n",
            "Current best accuracy (top-1 and 5 error): 29.6 3.18\n",
            "* Epoch: [29/300]\t Top 1-err 60.084  Top 5-err 12.684\t Train Loss 1.714\n",
            "* Epoch: [29/300]\t Top 1-err 31.270  Top 5-err 3.940\t Test Loss 1.016\n",
            "Current best accuracy (top-1 and 5 error): 29.6 3.18\n",
            "* Epoch: [30/300]\t Top 1-err 59.976  Top 5-err 12.586\t Train Loss 1.718\n",
            "* Epoch: [30/300]\t Top 1-err 29.220  Top 5-err 3.260\t Test Loss 0.962\n",
            "Current best accuracy (top-1 and 5 error): 29.22 3.26\n",
            "* Epoch: [31/300]\t Top 1-err 60.184  Top 5-err 12.656\t Train Loss 1.714\n",
            "* Epoch: [31/300]\t Top 1-err 31.050  Top 5-err 3.470\t Test Loss 1.016\n",
            "Current best accuracy (top-1 and 5 error): 29.22 3.26\n",
            "* Epoch: [32/300]\t Top 1-err 59.572  Top 5-err 12.514\t Train Loss 1.710\n",
            "* Epoch: [32/300]\t Top 1-err 28.350  Top 5-err 2.940\t Test Loss 0.954\n",
            "Current best accuracy (top-1 and 5 error): 28.35 2.94\n",
            "* Epoch: [33/300]\t Top 1-err 59.568  Top 5-err 12.062\t Train Loss 1.709\n",
            "* Epoch: [33/300]\t Top 1-err 30.960  Top 5-err 3.070\t Test Loss 0.960\n",
            "Current best accuracy (top-1 and 5 error): 28.35 2.94\n",
            "* Epoch: [34/300]\t Top 1-err 60.024  Top 5-err 12.686\t Train Loss 1.711\n",
            "* Epoch: [34/300]\t Top 1-err 30.310  Top 5-err 3.000\t Test Loss 0.998\n",
            "Current best accuracy (top-1 and 5 error): 28.35 2.94\n",
            "* Epoch: [35/300]\t Top 1-err 59.916  Top 5-err 12.324\t Train Loss 1.706\n",
            "* Epoch: [35/300]\t Top 1-err 28.790  Top 5-err 2.920\t Test Loss 0.967\n",
            "Current best accuracy (top-1 and 5 error): 28.35 2.94\n",
            "* Epoch: [36/300]\t Top 1-err 59.742  Top 5-err 12.330\t Train Loss 1.708\n",
            "* Epoch: [36/300]\t Top 1-err 29.860  Top 5-err 3.910\t Test Loss 1.005\n",
            "Current best accuracy (top-1 and 5 error): 28.35 2.94\n",
            "* Epoch: [37/300]\t Top 1-err 59.664  Top 5-err 12.348\t Train Loss 1.705\n",
            "* Epoch: [37/300]\t Top 1-err 31.310  Top 5-err 3.530\t Test Loss 1.026\n",
            "Current best accuracy (top-1 and 5 error): 28.35 2.94\n",
            "* Epoch: [38/300]\t Top 1-err 59.884  Top 5-err 12.438\t Train Loss 1.708\n",
            "* Epoch: [38/300]\t Top 1-err 31.990  Top 5-err 4.250\t Test Loss 1.007\n",
            "Current best accuracy (top-1 and 5 error): 28.35 2.94\n",
            "* Epoch: [39/300]\t Top 1-err 59.096  Top 5-err 12.354\t Train Loss 1.701\n",
            "* Epoch: [39/300]\t Top 1-err 28.010  Top 5-err 2.840\t Test Loss 0.927\n",
            "Current best accuracy (top-1 and 5 error): 28.01 2.84\n",
            "* Epoch: [40/300]\t Top 1-err 59.630  Top 5-err 12.322\t Train Loss 1.701\n",
            "* Epoch: [40/300]\t Top 1-err 30.030  Top 5-err 2.850\t Test Loss 0.986\n",
            "Current best accuracy (top-1 and 5 error): 28.01 2.84\n",
            "* Epoch: [41/300]\t Top 1-err 59.334  Top 5-err 12.468\t Train Loss 1.700\n",
            "* Epoch: [41/300]\t Top 1-err 30.620  Top 5-err 3.440\t Test Loss 1.004\n",
            "Current best accuracy (top-1 and 5 error): 28.01 2.84\n",
            "* Epoch: [42/300]\t Top 1-err 59.428  Top 5-err 12.184\t Train Loss 1.697\n",
            "* Epoch: [42/300]\t Top 1-err 30.290  Top 5-err 2.920\t Test Loss 0.965\n",
            "Current best accuracy (top-1 and 5 error): 28.01 2.84\n",
            "* Epoch: [43/300]\t Top 1-err 59.396  Top 5-err 12.492\t Train Loss 1.697\n",
            "* Epoch: [43/300]\t Top 1-err 29.110  Top 5-err 3.000\t Test Loss 0.959\n",
            "Current best accuracy (top-1 and 5 error): 28.01 2.84\n",
            "* Epoch: [44/300]\t Top 1-err 59.392  Top 5-err 11.916\t Train Loss 1.699\n",
            "* Epoch: [44/300]\t Top 1-err 27.280  Top 5-err 3.110\t Test Loss 0.960\n",
            "Current best accuracy (top-1 and 5 error): 27.28 3.11\n",
            "* Epoch: [45/300]\t Top 1-err 58.830  Top 5-err 11.990\t Train Loss 1.694\n",
            "* Epoch: [45/300]\t Top 1-err 30.090  Top 5-err 3.000\t Test Loss 0.940\n",
            "Current best accuracy (top-1 and 5 error): 27.28 3.11\n",
            "* Epoch: [46/300]\t Top 1-err 59.650  Top 5-err 12.212\t Train Loss 1.699\n",
            "* Epoch: [46/300]\t Top 1-err 26.130  Top 5-err 2.830\t Test Loss 0.896\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [47/300]\t Top 1-err 59.882  Top 5-err 12.240\t Train Loss 1.701\n",
            "* Epoch: [47/300]\t Top 1-err 28.370  Top 5-err 3.480\t Test Loss 0.933\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [48/300]\t Top 1-err 59.268  Top 5-err 12.080\t Train Loss 1.697\n",
            "* Epoch: [48/300]\t Top 1-err 28.870  Top 5-err 3.250\t Test Loss 0.963\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [49/300]\t Top 1-err 59.472  Top 5-err 12.300\t Train Loss 1.698\n",
            "* Epoch: [49/300]\t Top 1-err 27.050  Top 5-err 3.050\t Test Loss 0.906\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [50/300]\t Top 1-err 59.290  Top 5-err 11.872\t Train Loss 1.693\n",
            "* Epoch: [50/300]\t Top 1-err 29.880  Top 5-err 3.870\t Test Loss 0.963\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [51/300]\t Top 1-err 59.420  Top 5-err 12.026\t Train Loss 1.696\n",
            "* Epoch: [51/300]\t Top 1-err 28.110  Top 5-err 2.630\t Test Loss 0.924\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [52/300]\t Top 1-err 59.716  Top 5-err 12.376\t Train Loss 1.697\n",
            "* Epoch: [52/300]\t Top 1-err 28.520  Top 5-err 3.230\t Test Loss 0.939\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [53/300]\t Top 1-err 59.166  Top 5-err 12.348\t Train Loss 1.695\n",
            "* Epoch: [53/300]\t Top 1-err 30.210  Top 5-err 2.900\t Test Loss 0.961\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [54/300]\t Top 1-err 59.252  Top 5-err 12.184\t Train Loss 1.691\n",
            "* Epoch: [54/300]\t Top 1-err 29.170  Top 5-err 2.310\t Test Loss 0.951\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [55/300]\t Top 1-err 59.296  Top 5-err 12.106\t Train Loss 1.695\n",
            "* Epoch: [55/300]\t Top 1-err 28.480  Top 5-err 2.750\t Test Loss 0.951\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [56/300]\t Top 1-err 58.872  Top 5-err 12.018\t Train Loss 1.695\n",
            "* Epoch: [56/300]\t Top 1-err 27.160  Top 5-err 2.420\t Test Loss 0.886\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [57/300]\t Top 1-err 59.330  Top 5-err 11.772\t Train Loss 1.688\n",
            "* Epoch: [57/300]\t Top 1-err 28.130  Top 5-err 2.530\t Test Loss 0.932\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [58/300]\t Top 1-err 58.868  Top 5-err 11.888\t Train Loss 1.685\n",
            "* Epoch: [58/300]\t Top 1-err 28.510  Top 5-err 2.960\t Test Loss 0.936\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [59/300]\t Top 1-err 58.812  Top 5-err 11.896\t Train Loss 1.690\n",
            "* Epoch: [59/300]\t Top 1-err 27.580  Top 5-err 2.970\t Test Loss 0.926\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [60/300]\t Top 1-err 59.244  Top 5-err 12.186\t Train Loss 1.692\n",
            "* Epoch: [60/300]\t Top 1-err 29.150  Top 5-err 3.100\t Test Loss 0.954\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [61/300]\t Top 1-err 59.302  Top 5-err 11.868\t Train Loss 1.693\n",
            "* Epoch: [61/300]\t Top 1-err 28.060  Top 5-err 3.230\t Test Loss 0.944\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [62/300]\t Top 1-err 58.808  Top 5-err 11.948\t Train Loss 1.691\n",
            "* Epoch: [62/300]\t Top 1-err 27.870  Top 5-err 3.150\t Test Loss 0.932\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [63/300]\t Top 1-err 59.036  Top 5-err 12.068\t Train Loss 1.689\n",
            "* Epoch: [63/300]\t Top 1-err 31.420  Top 5-err 2.860\t Test Loss 0.967\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [64/300]\t Top 1-err 58.518  Top 5-err 11.836\t Train Loss 1.689\n",
            "* Epoch: [64/300]\t Top 1-err 28.820  Top 5-err 2.890\t Test Loss 0.941\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [65/300]\t Top 1-err 59.180  Top 5-err 11.994\t Train Loss 1.688\n",
            "* Epoch: [65/300]\t Top 1-err 27.960  Top 5-err 2.640\t Test Loss 0.916\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [66/300]\t Top 1-err 59.050  Top 5-err 11.874\t Train Loss 1.688\n",
            "* Epoch: [66/300]\t Top 1-err 30.250  Top 5-err 3.330\t Test Loss 0.981\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [67/300]\t Top 1-err 59.548  Top 5-err 11.934\t Train Loss 1.686\n",
            "* Epoch: [67/300]\t Top 1-err 27.690  Top 5-err 2.410\t Test Loss 0.882\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [68/300]\t Top 1-err 58.680  Top 5-err 12.104\t Train Loss 1.686\n",
            "* Epoch: [68/300]\t Top 1-err 28.410  Top 5-err 3.130\t Test Loss 0.933\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [69/300]\t Top 1-err 58.950  Top 5-err 11.426\t Train Loss 1.684\n",
            "* Epoch: [69/300]\t Top 1-err 27.390  Top 5-err 2.920\t Test Loss 0.915\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [70/300]\t Top 1-err 59.028  Top 5-err 11.782\t Train Loss 1.687\n",
            "* Epoch: [70/300]\t Top 1-err 29.260  Top 5-err 3.080\t Test Loss 0.962\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [71/300]\t Top 1-err 59.176  Top 5-err 11.792\t Train Loss 1.687\n",
            "* Epoch: [71/300]\t Top 1-err 29.230  Top 5-err 3.120\t Test Loss 0.958\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [72/300]\t Top 1-err 58.894  Top 5-err 11.748\t Train Loss 1.684\n",
            "* Epoch: [72/300]\t Top 1-err 26.940  Top 5-err 3.000\t Test Loss 0.895\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [73/300]\t Top 1-err 59.092  Top 5-err 12.106\t Train Loss 1.685\n",
            "* Epoch: [73/300]\t Top 1-err 29.680  Top 5-err 3.500\t Test Loss 0.948\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [74/300]\t Top 1-err 58.892  Top 5-err 11.778\t Train Loss 1.685\n",
            "* Epoch: [74/300]\t Top 1-err 27.520  Top 5-err 2.860\t Test Loss 0.909\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [75/300]\t Top 1-err 59.270  Top 5-err 12.072\t Train Loss 1.687\n",
            "* Epoch: [75/300]\t Top 1-err 27.720  Top 5-err 2.630\t Test Loss 0.937\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [76/300]\t Top 1-err 58.582  Top 5-err 11.856\t Train Loss 1.680\n",
            "* Epoch: [76/300]\t Top 1-err 29.270  Top 5-err 3.000\t Test Loss 0.966\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [77/300]\t Top 1-err 58.800  Top 5-err 11.770\t Train Loss 1.686\n",
            "* Epoch: [77/300]\t Top 1-err 30.890  Top 5-err 2.490\t Test Loss 0.964\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [78/300]\t Top 1-err 58.972  Top 5-err 11.854\t Train Loss 1.682\n",
            "* Epoch: [78/300]\t Top 1-err 29.040  Top 5-err 3.690\t Test Loss 0.950\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [79/300]\t Top 1-err 59.046  Top 5-err 12.296\t Train Loss 1.689\n",
            "* Epoch: [79/300]\t Top 1-err 31.570  Top 5-err 3.420\t Test Loss 0.986\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [80/300]\t Top 1-err 58.882  Top 5-err 11.816\t Train Loss 1.680\n",
            "* Epoch: [80/300]\t Top 1-err 30.270  Top 5-err 3.360\t Test Loss 0.950\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [81/300]\t Top 1-err 58.916  Top 5-err 11.806\t Train Loss 1.680\n",
            "* Epoch: [81/300]\t Top 1-err 28.560  Top 5-err 2.630\t Test Loss 0.936\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [82/300]\t Top 1-err 58.872  Top 5-err 11.752\t Train Loss 1.681\n",
            "* Epoch: [82/300]\t Top 1-err 27.480  Top 5-err 2.930\t Test Loss 0.921\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [83/300]\t Top 1-err 58.594  Top 5-err 11.810\t Train Loss 1.681\n",
            "* Epoch: [83/300]\t Top 1-err 29.790  Top 5-err 3.200\t Test Loss 0.958\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [84/300]\t Top 1-err 58.990  Top 5-err 11.772\t Train Loss 1.679\n",
            "* Epoch: [84/300]\t Top 1-err 27.980  Top 5-err 3.390\t Test Loss 0.946\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [85/300]\t Top 1-err 58.968  Top 5-err 11.982\t Train Loss 1.682\n",
            "* Epoch: [85/300]\t Top 1-err 28.420  Top 5-err 3.190\t Test Loss 0.935\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [86/300]\t Top 1-err 58.622  Top 5-err 11.570\t Train Loss 1.679\n",
            "* Epoch: [86/300]\t Top 1-err 30.230  Top 5-err 2.890\t Test Loss 0.987\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [87/300]\t Top 1-err 58.662  Top 5-err 11.628\t Train Loss 1.682\n",
            "* Epoch: [87/300]\t Top 1-err 29.050  Top 5-err 2.980\t Test Loss 0.928\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [88/300]\t Top 1-err 59.150  Top 5-err 11.906\t Train Loss 1.680\n",
            "* Epoch: [88/300]\t Top 1-err 28.480  Top 5-err 2.660\t Test Loss 0.907\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [89/300]\t Top 1-err 58.764  Top 5-err 12.060\t Train Loss 1.683\n",
            "* Epoch: [89/300]\t Top 1-err 27.950  Top 5-err 3.030\t Test Loss 0.931\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [90/300]\t Top 1-err 58.844  Top 5-err 11.798\t Train Loss 1.679\n",
            "* Epoch: [90/300]\t Top 1-err 28.190  Top 5-err 3.200\t Test Loss 0.922\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [91/300]\t Top 1-err 59.080  Top 5-err 11.934\t Train Loss 1.684\n",
            "* Epoch: [91/300]\t Top 1-err 29.420  Top 5-err 3.680\t Test Loss 0.966\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [92/300]\t Top 1-err 58.954  Top 5-err 11.606\t Train Loss 1.678\n",
            "* Epoch: [92/300]\t Top 1-err 27.180  Top 5-err 3.000\t Test Loss 0.917\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [93/300]\t Top 1-err 58.856  Top 5-err 11.714\t Train Loss 1.678\n",
            "* Epoch: [93/300]\t Top 1-err 32.480  Top 5-err 3.380\t Test Loss 0.990\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [94/300]\t Top 1-err 58.956  Top 5-err 11.772\t Train Loss 1.684\n",
            "* Epoch: [94/300]\t Top 1-err 27.050  Top 5-err 3.040\t Test Loss 0.910\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [95/300]\t Top 1-err 58.934  Top 5-err 11.686\t Train Loss 1.681\n",
            "* Epoch: [95/300]\t Top 1-err 27.900  Top 5-err 3.040\t Test Loss 0.895\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [96/300]\t Top 1-err 58.700  Top 5-err 11.786\t Train Loss 1.678\n",
            "* Epoch: [96/300]\t Top 1-err 32.200  Top 5-err 4.910\t Test Loss 1.030\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [97/300]\t Top 1-err 58.720  Top 5-err 11.498\t Train Loss 1.680\n",
            "* Epoch: [97/300]\t Top 1-err 31.320  Top 5-err 3.610\t Test Loss 0.991\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [98/300]\t Top 1-err 58.958  Top 5-err 11.970\t Train Loss 1.679\n",
            "* Epoch: [98/300]\t Top 1-err 28.470  Top 5-err 3.340\t Test Loss 0.918\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [99/300]\t Top 1-err 58.816  Top 5-err 11.456\t Train Loss 1.677\n",
            "* Epoch: [99/300]\t Top 1-err 29.020  Top 5-err 2.860\t Test Loss 0.940\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [100/300]\t Top 1-err 59.216  Top 5-err 11.960\t Train Loss 1.682\n",
            "* Epoch: [100/300]\t Top 1-err 27.650  Top 5-err 2.670\t Test Loss 0.901\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [101/300]\t Top 1-err 58.766  Top 5-err 11.678\t Train Loss 1.681\n",
            "* Epoch: [101/300]\t Top 1-err 26.410  Top 5-err 2.810\t Test Loss 0.919\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [102/300]\t Top 1-err 58.762  Top 5-err 11.612\t Train Loss 1.679\n",
            "* Epoch: [102/300]\t Top 1-err 29.190  Top 5-err 2.720\t Test Loss 0.975\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [103/300]\t Top 1-err 58.632  Top 5-err 11.708\t Train Loss 1.676\n",
            "* Epoch: [103/300]\t Top 1-err 26.240  Top 5-err 2.500\t Test Loss 0.895\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [104/300]\t Top 1-err 59.294  Top 5-err 11.970\t Train Loss 1.682\n",
            "* Epoch: [104/300]\t Top 1-err 28.630  Top 5-err 2.870\t Test Loss 0.938\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [105/300]\t Top 1-err 58.646  Top 5-err 11.664\t Train Loss 1.680\n",
            "* Epoch: [105/300]\t Top 1-err 27.660  Top 5-err 2.840\t Test Loss 0.928\n",
            "Current best accuracy (top-1 and 5 error): 26.13 2.83\n",
            "* Epoch: [106/300]\t Top 1-err 58.580  Top 5-err 11.468\t Train Loss 1.677\n",
            "* Epoch: [106/300]\t Top 1-err 25.650  Top 5-err 2.770\t Test Loss 0.895\n",
            "Current best accuracy (top-1 and 5 error): 25.65 2.77\n",
            "* Epoch: [107/300]\t Top 1-err 58.956  Top 5-err 11.616\t Train Loss 1.676\n",
            "* Epoch: [107/300]\t Top 1-err 28.080  Top 5-err 3.540\t Test Loss 0.923\n",
            "Current best accuracy (top-1 and 5 error): 25.65 2.77\n",
            "* Epoch: [108/300]\t Top 1-err 58.526  Top 5-err 11.530\t Train Loss 1.673\n",
            "* Epoch: [108/300]\t Top 1-err 27.670  Top 5-err 3.040\t Test Loss 0.957\n",
            "Current best accuracy (top-1 and 5 error): 25.65 2.77\n",
            "* Epoch: [109/300]\t Top 1-err 58.964  Top 5-err 11.578\t Train Loss 1.673\n",
            "* Epoch: [109/300]\t Top 1-err 28.460  Top 5-err 2.990\t Test Loss 0.949\n",
            "Current best accuracy (top-1 and 5 error): 25.65 2.77\n",
            "* Epoch: [110/300]\t Top 1-err 58.806  Top 5-err 11.434\t Train Loss 1.674\n",
            "* Epoch: [110/300]\t Top 1-err 28.980  Top 5-err 2.660\t Test Loss 0.964\n",
            "Current best accuracy (top-1 and 5 error): 25.65 2.77\n",
            "* Epoch: [111/300]\t Top 1-err 58.786  Top 5-err 11.642\t Train Loss 1.673\n",
            "* Epoch: [111/300]\t Top 1-err 27.410  Top 5-err 3.200\t Test Loss 0.940\n",
            "Current best accuracy (top-1 and 5 error): 25.65 2.77\n",
            "* Epoch: [112/300]\t Top 1-err 58.518  Top 5-err 11.736\t Train Loss 1.676\n",
            "* Epoch: [112/300]\t Top 1-err 27.550  Top 5-err 2.950\t Test Loss 0.931\n",
            "Current best accuracy (top-1 and 5 error): 25.65 2.77\n",
            "* Epoch: [113/300]\t Top 1-err 58.834  Top 5-err 11.490\t Train Loss 1.678\n",
            "* Epoch: [113/300]\t Top 1-err 27.680  Top 5-err 3.360\t Test Loss 0.954\n",
            "Current best accuracy (top-1 and 5 error): 25.65 2.77\n",
            "* Epoch: [114/300]\t Top 1-err 58.816  Top 5-err 11.606\t Train Loss 1.680\n",
            "* Epoch: [114/300]\t Top 1-err 31.090  Top 5-err 3.300\t Test Loss 0.978\n",
            "Current best accuracy (top-1 and 5 error): 25.65 2.77\n",
            "* Epoch: [115/300]\t Top 1-err 58.338  Top 5-err 11.456\t Train Loss 1.672\n",
            "* Epoch: [115/300]\t Top 1-err 25.500  Top 5-err 2.650\t Test Loss 0.881\n",
            "Current best accuracy (top-1 and 5 error): 25.5 2.65\n",
            "* Epoch: [116/300]\t Top 1-err 58.540  Top 5-err 11.616\t Train Loss 1.674\n",
            "* Epoch: [116/300]\t Top 1-err 28.800  Top 5-err 3.100\t Test Loss 0.997\n",
            "Current best accuracy (top-1 and 5 error): 25.5 2.65\n",
            "* Epoch: [117/300]\t Top 1-err 58.532  Top 5-err 11.820\t Train Loss 1.677\n",
            "* Epoch: [117/300]\t Top 1-err 28.900  Top 5-err 2.440\t Test Loss 0.935\n",
            "Current best accuracy (top-1 and 5 error): 25.5 2.65\n",
            "* Epoch: [118/300]\t Top 1-err 58.704  Top 5-err 11.422\t Train Loss 1.672\n",
            "* Epoch: [118/300]\t Top 1-err 26.800  Top 5-err 2.560\t Test Loss 0.931\n",
            "Current best accuracy (top-1 and 5 error): 25.5 2.65\n",
            "* Epoch: [119/300]\t Top 1-err 58.686  Top 5-err 11.500\t Train Loss 1.674\n",
            "* Epoch: [119/300]\t Top 1-err 28.190  Top 5-err 2.950\t Test Loss 0.932\n",
            "Current best accuracy (top-1 and 5 error): 25.5 2.65\n",
            "* Epoch: [120/300]\t Top 1-err 58.774  Top 5-err 11.538\t Train Loss 1.673\n",
            "* Epoch: [120/300]\t Top 1-err 27.270  Top 5-err 2.510\t Test Loss 0.931\n",
            "Current best accuracy (top-1 and 5 error): 25.5 2.65\n",
            "* Epoch: [121/300]\t Top 1-err 58.492  Top 5-err 11.620\t Train Loss 1.670\n",
            "* Epoch: [121/300]\t Top 1-err 29.480  Top 5-err 3.020\t Test Loss 0.941\n",
            "Current best accuracy (top-1 and 5 error): 25.5 2.65\n",
            "* Epoch: [122/300]\t Top 1-err 58.752  Top 5-err 11.662\t Train Loss 1.673\n",
            "* Epoch: [122/300]\t Top 1-err 28.310  Top 5-err 3.340\t Test Loss 0.937\n",
            "Current best accuracy (top-1 and 5 error): 25.5 2.65\n",
            "* Epoch: [123/300]\t Top 1-err 58.088  Top 5-err 11.698\t Train Loss 1.670\n",
            "* Epoch: [123/300]\t Top 1-err 24.810  Top 5-err 2.600\t Test Loss 0.892\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [124/300]\t Top 1-err 58.706  Top 5-err 11.782\t Train Loss 1.672\n",
            "* Epoch: [124/300]\t Top 1-err 26.050  Top 5-err 2.760\t Test Loss 0.884\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [125/300]\t Top 1-err 58.240  Top 5-err 11.416\t Train Loss 1.669\n",
            "* Epoch: [125/300]\t Top 1-err 28.020  Top 5-err 3.090\t Test Loss 0.914\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [126/300]\t Top 1-err 58.286  Top 5-err 11.522\t Train Loss 1.670\n",
            "* Epoch: [126/300]\t Top 1-err 31.520  Top 5-err 4.210\t Test Loss 1.000\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [127/300]\t Top 1-err 58.796  Top 5-err 11.746\t Train Loss 1.675\n",
            "* Epoch: [127/300]\t Top 1-err 27.510  Top 5-err 2.880\t Test Loss 0.928\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [128/300]\t Top 1-err 58.624  Top 5-err 11.626\t Train Loss 1.674\n",
            "* Epoch: [128/300]\t Top 1-err 30.590  Top 5-err 3.670\t Test Loss 0.956\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [129/300]\t Top 1-err 58.902  Top 5-err 11.744\t Train Loss 1.670\n",
            "* Epoch: [129/300]\t Top 1-err 28.280  Top 5-err 3.660\t Test Loss 0.934\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [130/300]\t Top 1-err 58.096  Top 5-err 11.710\t Train Loss 1.671\n",
            "* Epoch: [130/300]\t Top 1-err 26.070  Top 5-err 2.450\t Test Loss 0.856\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [131/300]\t Top 1-err 58.606  Top 5-err 11.164\t Train Loss 1.673\n",
            "* Epoch: [131/300]\t Top 1-err 27.360  Top 5-err 2.990\t Test Loss 0.896\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [132/300]\t Top 1-err 58.542  Top 5-err 11.756\t Train Loss 1.676\n",
            "* Epoch: [132/300]\t Top 1-err 27.040  Top 5-err 2.520\t Test Loss 0.930\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [133/300]\t Top 1-err 58.208  Top 5-err 11.506\t Train Loss 1.676\n",
            "* Epoch: [133/300]\t Top 1-err 25.540  Top 5-err 2.430\t Test Loss 0.855\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [134/300]\t Top 1-err 58.566  Top 5-err 11.716\t Train Loss 1.669\n",
            "* Epoch: [134/300]\t Top 1-err 27.910  Top 5-err 3.050\t Test Loss 0.930\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [135/300]\t Top 1-err 58.574  Top 5-err 11.502\t Train Loss 1.676\n",
            "* Epoch: [135/300]\t Top 1-err 26.220  Top 5-err 2.880\t Test Loss 0.899\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [136/300]\t Top 1-err 58.314  Top 5-err 11.458\t Train Loss 1.668\n",
            "* Epoch: [136/300]\t Top 1-err 27.850  Top 5-err 3.280\t Test Loss 0.915\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [137/300]\t Top 1-err 58.806  Top 5-err 11.670\t Train Loss 1.673\n",
            "* Epoch: [137/300]\t Top 1-err 26.680  Top 5-err 3.120\t Test Loss 0.903\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [138/300]\t Top 1-err 58.804  Top 5-err 11.718\t Train Loss 1.671\n",
            "* Epoch: [138/300]\t Top 1-err 27.270  Top 5-err 3.120\t Test Loss 0.906\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [139/300]\t Top 1-err 58.730  Top 5-err 11.726\t Train Loss 1.675\n",
            "* Epoch: [139/300]\t Top 1-err 29.100  Top 5-err 3.220\t Test Loss 0.944\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [140/300]\t Top 1-err 58.782  Top 5-err 11.584\t Train Loss 1.674\n",
            "* Epoch: [140/300]\t Top 1-err 30.200  Top 5-err 2.770\t Test Loss 0.968\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [141/300]\t Top 1-err 58.440  Top 5-err 11.544\t Train Loss 1.670\n",
            "* Epoch: [141/300]\t Top 1-err 26.880  Top 5-err 2.960\t Test Loss 0.888\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [142/300]\t Top 1-err 58.688  Top 5-err 11.684\t Train Loss 1.669\n",
            "* Epoch: [142/300]\t Top 1-err 27.740  Top 5-err 2.580\t Test Loss 0.933\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [143/300]\t Top 1-err 58.740  Top 5-err 11.718\t Train Loss 1.674\n",
            "* Epoch: [143/300]\t Top 1-err 26.460  Top 5-err 2.740\t Test Loss 0.901\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [144/300]\t Top 1-err 58.870  Top 5-err 11.642\t Train Loss 1.668\n",
            "* Epoch: [144/300]\t Top 1-err 27.860  Top 5-err 2.930\t Test Loss 0.910\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [145/300]\t Top 1-err 58.784  Top 5-err 11.420\t Train Loss 1.669\n",
            "* Epoch: [145/300]\t Top 1-err 25.430  Top 5-err 2.650\t Test Loss 0.873\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [146/300]\t Top 1-err 58.592  Top 5-err 11.482\t Train Loss 1.667\n",
            "* Epoch: [146/300]\t Top 1-err 26.720  Top 5-err 2.470\t Test Loss 0.911\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [147/300]\t Top 1-err 58.496  Top 5-err 11.646\t Train Loss 1.670\n",
            "* Epoch: [147/300]\t Top 1-err 27.920  Top 5-err 2.790\t Test Loss 0.903\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [148/300]\t Top 1-err 58.514  Top 5-err 11.562\t Train Loss 1.672\n",
            "* Epoch: [148/300]\t Top 1-err 30.330  Top 5-err 3.610\t Test Loss 0.983\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [149/300]\t Top 1-err 58.378  Top 5-err 11.210\t Train Loss 1.666\n",
            "* Epoch: [149/300]\t Top 1-err 27.740  Top 5-err 3.220\t Test Loss 0.922\n",
            "Current best accuracy (top-1 and 5 error): 24.81 2.6\n",
            "* Epoch: [150/300]\t Top 1-err 56.286  Top 5-err 9.426\t Train Loss 1.571\n",
            "* Epoch: [150/300]\t Top 1-err 22.230  Top 5-err 2.020\t Test Loss 0.769\n",
            "Current best accuracy (top-1 and 5 error): 22.23 2.02\n",
            "* Epoch: [151/300]\t Top 1-err 55.190  Top 5-err 8.872\t Train Loss 1.540\n",
            "* Epoch: [151/300]\t Top 1-err 22.150  Top 5-err 2.040\t Test Loss 0.768\n",
            "Current best accuracy (top-1 and 5 error): 22.15 2.04\n",
            "* Epoch: [152/300]\t Top 1-err 55.020  Top 5-err 8.520\t Train Loss 1.538\n",
            "* Epoch: [152/300]\t Top 1-err 22.110  Top 5-err 2.030\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.11 2.03\n",
            "* Epoch: [153/300]\t Top 1-err 54.676  Top 5-err 8.548\t Train Loss 1.539\n",
            "* Epoch: [153/300]\t Top 1-err 22.100  Top 5-err 2.030\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.1 2.03\n",
            "* Epoch: [154/300]\t Top 1-err 55.210  Top 5-err 8.756\t Train Loss 1.538\n",
            "* Epoch: [154/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [155/300]\t Top 1-err 55.460  Top 5-err 8.674\t Train Loss 1.538\n",
            "* Epoch: [155/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [156/300]\t Top 1-err 54.986  Top 5-err 8.694\t Train Loss 1.543\n",
            "* Epoch: [156/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [157/300]\t Top 1-err 55.054  Top 5-err 8.440\t Train Loss 1.535\n",
            "* Epoch: [157/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [158/300]\t Top 1-err 55.362  Top 5-err 8.636\t Train Loss 1.541\n",
            "* Epoch: [158/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [159/300]\t Top 1-err 55.208  Top 5-err 8.826\t Train Loss 1.537\n",
            "* Epoch: [159/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [160/300]\t Top 1-err 55.194  Top 5-err 8.652\t Train Loss 1.533\n",
            "* Epoch: [160/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [161/300]\t Top 1-err 55.280  Top 5-err 8.582\t Train Loss 1.538\n",
            "* Epoch: [161/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [162/300]\t Top 1-err 55.032  Top 5-err 8.792\t Train Loss 1.542\n",
            "* Epoch: [162/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [163/300]\t Top 1-err 55.622  Top 5-err 8.728\t Train Loss 1.540\n",
            "* Epoch: [163/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [164/300]\t Top 1-err 55.284  Top 5-err 8.574\t Train Loss 1.540\n",
            "* Epoch: [164/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [165/300]\t Top 1-err 54.806  Top 5-err 8.410\t Train Loss 1.533\n",
            "* Epoch: [165/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [166/300]\t Top 1-err 54.776  Top 5-err 8.550\t Train Loss 1.536\n",
            "* Epoch: [166/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [167/300]\t Top 1-err 54.798  Top 5-err 8.532\t Train Loss 1.534\n",
            "* Epoch: [167/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [168/300]\t Top 1-err 55.528  Top 5-err 8.606\t Train Loss 1.534\n",
            "* Epoch: [168/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [169/300]\t Top 1-err 55.174  Top 5-err 8.866\t Train Loss 1.537\n",
            "* Epoch: [169/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [170/300]\t Top 1-err 54.996  Top 5-err 8.666\t Train Loss 1.539\n",
            "* Epoch: [170/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [171/300]\t Top 1-err 54.938  Top 5-err 8.780\t Train Loss 1.540\n",
            "* Epoch: [171/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [172/300]\t Top 1-err 54.984  Top 5-err 8.444\t Train Loss 1.536\n",
            "* Epoch: [172/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [173/300]\t Top 1-err 54.680  Top 5-err 8.634\t Train Loss 1.536\n",
            "* Epoch: [173/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [174/300]\t Top 1-err 54.630  Top 5-err 8.470\t Train Loss 1.536\n",
            "* Epoch: [174/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n",
            "* Epoch: [175/300]\t Top 1-err 54.922  Top 5-err 8.394\t Train Loss 1.536\n",
            "* Epoch: [175/300]\t Top 1-err 22.090  Top 5-err 2.020\t Test Loss 0.767\n",
            "Current best accuracy (top-1 and 5 error): 22.09 2.02\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-767df340adeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-f441c9685347>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcurrent_LR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# measure data loading time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdata_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ao41DJ37-6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu35amGg7_c7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}